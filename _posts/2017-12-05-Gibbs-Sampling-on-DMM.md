---
layout: post
title: "Gibbs Sampling on Dirichlet Multinomial Mixture Models"
date: 2017-12-05
mathjax: true
categories: [Bayesian Inference]
---

### Key Concepts
* Gibbs Sampling is a type of MCMC method, which allows us to obtain samples from probability distributions, without having to explicitly calculate the values for their marginalizing integrals. The key idea (amongst the other MCMC methods) is to sample each unknown variable in turn, condition on the value of all other variables in the model.

* The Dirichlet Multinomial Mixture model is a frequently encountered model in Bayesian Statistics. Here a prior $\theta$ is drawn from a dirichlet distribution, with parameters $\gamma_\theta$, $\theta \sim Dir(\gamma_\theta)$, and the observed values are drawn from a Multinomial distribution with parameters $\theta$ and trials $n$, $V \sim Multinomial(\theta, n)$.

* The Dirichlet distribution is a conjugate distribution to the Multinomial distribution, which has useful properties in the context of Gibbs Sampling.

### Model Preliminaries
In Bayesian Inference, the aim is to infer the posterior probability distribution over a set of random variables. However this involves computing integrals which are mostly intractable.

MCMC methods are a way to approximate the value of the integral, by sampling from the joint distribution. (Hence, we never need to calculate explicitly the value of the integral). The idea is to sample from a state space of $Z$ such that the likelihood of sampling $z$ is proportional to $p(z)$. The Markov Chain indicates that the next state, $t+1$, that we visit depends only on the current state, $t$. That is,

\begin{equation}
P_{trans}(z^{t+1}\|z^{t}) = P_{trans}(z^{t+1}\|z^t, z^{t-1}, ..., z^1)
\end{equation}

MCMC methods try to approximate $P_{trans}(z^{t+1}\|z^{t})$ such that the probability of visiting $z$ is proportional to $p(z)$. Values of variables in the model can be approximated from simple statistics. Read more about MCMC [here]. 

**Gibbs Sampling** is one type of MCMC method, that applies when $z$ has $k$ dimensions, $z=<z_1, z_2, ..., z_k>$ . Each dimension corresponds to a parameter or variable in our model. The algorithm samples from the conditional distribution by updating $z$ one dimension at a time. The probabilistic walk samples each $k$ dimension, condition on the other $k-1$ dimensions. In this process, new values for the variables are used as soon as they are obtained. For each iteration $t$ in $1..T$, sample each $i$ in $1..k$, such that:

\begin{equation}
 z_i^{t+1} = P(z_i^{t+1}\|z_1^{t+1}, ..., z_{i-1}^{t+1}, z_{i+1}^{t},..., z_k^{t})
\end{equation}

In Probabilistic Graphical models, we seek to infer(through Bayesian inference) probability distributions over latent variables which are parameters in the model. To apply Gibbs Sampling, we need to first obtain the mathematical equations for the conditional updates. The general workflow is as follows:

1. **Assume a generative model** which allows us to observe the data generated by some latent variables. The model is often constructed with domain knowledge and results in structured joint distributions.
2. **Factorise the joint distribution** of the model to conditional distribution based on the model structure.
3. Express the conditional distributions in terms of  **mathematical equations based on the type of distribution.**
4. Obtain **equations for conditional update** of variables.
5. **Sample variables** by running the Gibbs Sampling algorithm.
6. **Obtain approximate values** of the variables via sample statistics.


### Implementing Gibbs Sampling on Dirichlet Multinomial Mixture Model
The rest of this document follows the Dirichlet Multinomial Model from "Gibbs Sampling for the Uninitiated". The model described in the paper has documents as the item of interest. Each document, $doc_j$, can be represented as a bag of words, $W_j$. The observed features are the words in the document, with the k-th word in the j-th document represented as $W_{jk}$. The goal is to predict a sentiment label $L_j$ for each of the documents, after observing $W_j$. By Bayes Rule,

\begin{equation}
L_j = argmax_LP(L\|W_j) = argmax_L\frac{P(W_j\|L)P(L)}{P(W_j)}
\end{equation}

#### <span style="color:blue">**1. Assume a generative model of text**</span>

Then, $L_j = argmax_LP(L\|W_j, \pi, \gamma_\pi, \theta, \gamma_\theta)$

* L is a 2-class label, which can be sampled from a Bernoulli distribution with parameter $\pi$ where $P(L_j=1)$. For more than 2 classes, L can be sampled from a multinomial distribution.

\begin{equation}
L_j \sim Bernoulli(\pi) 
\end{equation}

* $\pi$, the parameter of the Bernoulli distribution, is sampled from a prior probability - the Beta distribution which has hyperparameter $\gamma_\pi = [\alpha, \beta]$. Without any previous information, we use an *uninformative prior*,  $\gamma_\pi = [1, 1]$, which returns a uniform distribution where any value of $\pi$ is equally likely.

\begin{equation}
\pi \sim Beta(\gamma_\pi)
\end{equation}

* We assumes a Naive model of word generation, where the words generated are independent of each other given L. The probability distribution that the words are sampled from depends on whether $L_j$ is 0 or 1. If $L_j=0$, the probability distribution we sample from is $w_0 \sim Multinomial(\theta_0)$. Generally, 

\begin{equation}
w_j \sim Multinomial(\theta_{L_j})
\end{equation}

* $\theta_{L_j}$, the parameter of the Multinomial distribution is sampled from a prior probability, the Dirichlet distribution. The Dirichlet distribution is the Beta distribution generalized to more than two possible values. The dirichlet distribution is parameterised by hyperparameter $\gamma_\theta$. The number of dimensions is the number of words in the vocabulary. $\gamma_\theta = [\gamma_{\theta_1}, \gamma_{\theta_2},...,\gamma_{\theta_v}]$

\begin{equation}
\theta \sim Dirichlet(\gamma_\theta)
\end{equation}
![Fig1](/assets/DMM-Fig1.png)
