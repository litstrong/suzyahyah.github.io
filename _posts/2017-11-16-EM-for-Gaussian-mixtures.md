---
layout: post
title:  "EM Algorithm for Gaussian mixtures"
date:   2017-11-16 21:09:09 +0800
mathjax: true
categories: jekyll update
---
### Key Concepts
* Gaussian Mixture Model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions, with unknown parameters
* Expectation Maximization is an iterative algorithm that classifies data points with the model, and updates parameters with the newly classified data, and classifies data points with the new parameters.
* We use Expectation Maximization to learn the parameters of the GMM in an unsupervised fashion.

### Model Preliminaries
Datapoints are generated in an IID fashion from an underlying density \$ p(X) \$. 
\$ p(X) \$ is defined as a finite mixture model with K components:

$$ p(X|\theta)=\sum_{k=1}^{K}\alpha_{k}.p_{k}(x|z_k, \theta_{k}) $$

Assuming our finite mixture model is a Gaussian mixture model, we can model \$ p_{k}(x\|\theta_k) \$ as a Gaussian density that has parameters $ \theta_k = \{\mu_k, \Sigma_k\} $

$$ p_k(x|\theta_k) = \frac{1}{(2\pi)^{d/2}|\Sigma_k|^{1/2}}.e^{(-\frac{1}{2}(x-\mu_k)^t\Sigma_k^{-1}(x-\mu_k))} $$

$\alpha_k$ are mixture weights, representing the underlying probability of component k. i.e. the probability that a randomly selected X was generated by component $k$, where $\sum_{k=1}^{K} \alpha_k = 1.

Given a new datapoint, we want to compute the "membership weight", $w_{ik}$ of the datapoint $x_i$ in cluster $k$, given the parameters $\theta$.

$$ w_{ik} = \frac{p_k(x_i|z_k, \theta_k).\alpha_k}{\sum_{m=1}^{K}p_m(x_i|z_m, \theta_m).\alpha_m}$$ 

### EM Algorithm
The algorithm cycles between an E and M step, and iteratively updates $\theta$ until convergence is detected. Initially, either the membership weights can be initialised randomly (leading with M step) or model parameters initialised randomly (leading with E step).

*E-step*: Compute the membership weights $w_{ik}$ for all data points, and all mixture components. E
*M-step*: Use the membership weights and the data to calculate new parameter values. 
1. Calculating mixture weights
$$ \alpha_k^{new} = \frac{N_k}{N}, 1 \leq k \leq K $$
2. Calculate mean $\mu_k$, and variance $\Sigma_k$ of the mixture component

$$ \mu_k^{new} = (\frac{1}{N_k})\sum_{i=1}^{N}w_{ik}.x_i, 1 \leq k \leq K $$
$$ \Sigma_k^{new} = (\frac{1}{N_k})\sum_{i=1}^{N}w_{ik}.(x_i - \mu_k^{new})(x_i - \mu_k^{new})^t$$

After computing the parameters, we can recompute the membership weights with the E-step, and you get the idea. Repeat until convergence.
 
### Convergence of the EM Algorithm
Convergence is when there is no significant change (or below threshold) for the log-likelihood after each iteration. Log likelihood is given by:

$$ \sum_{i=1}^{N}logp(x_i|\theta) = \sum_{i=1}^{N}(log\sum_{k=1}^{K}\alpha_{k}p_k(x_i|z_k, \theta_k)) $$

Thats it, thanks for reading!
