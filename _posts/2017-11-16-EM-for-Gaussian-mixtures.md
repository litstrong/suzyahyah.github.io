---
layout: post
title:  "EM Algorithm for Gaussian mixtures"
date:   2017-11-16 21:09:09 +0800
mathjax: true
categories: jekyll update
---
### Key Concepts
* Gaussian Mixture Model is a probabilistic model that assumes all the data points are generated from a mixture of a finite number of Gaussian distributions, with unknown parameters
* Expectation Maximization is an iterative algorithm that classifies data points with the model, updates model parameters with the newly classified data, and subsequently classifies data points with the new parameters.
* We use Expectation Maximization to learn the parameters of the GMM in an unsupervised fashion.

### Model Preliminaries
Datapoints are generated in an IID fashion from an underlying density \$ p(X) \$. 
\$ p(X) \$ is defined as a finite mixture model with K components:

$$ p(X|\theta)=\sum_{k=1}^{K}\alpha_{k}.p_{k}(x|z_k, \theta_{k}) $$

Assuming our finite mixture model is a Gaussian mixture model, we can model \$ p_{k}(x\|\theta_k) \$ as a Gaussian density that has parameters $ \theta_k = \{\mu_k, \Sigma_k\} $

$$ p_k(x|\theta_k) = \frac{1}{(2\pi)^{d/2}|\Sigma_k|^{1/2}}.e^{(-\frac{1}{2}(x-\mu_k)^t\Sigma_k^{-1}(x-\mu_k))} $$

$\alpha_k$ are mixture weights, representing the underlying probability of component k. i.e. the probability that a randomly selected X was generated by component $k$, where $\sum_{k=1}^{K} \alpha_k = 1 $.

Given a new datapoint, we want to compute the "membership weight", $w_{ik}$ of the datapoint $x_i$ in cluster $k$, given the parameters $\theta$. Membership weights for a single datapoint should sum to 1.

### Implementing the EM algorithm for Gaussian Mixture Models
The algorithm cycles between an E and M step, and iteratively updates $\theta$ until convergence is detected. Initially, either the membership weights can be initialised randomly (leading with M step) or model parameters initialised randomly (leading with E step).

We will need:
1. E-step
2. M-step
3. EM algorithm (E-step + M-step)
4. The log likelihood function for assessing model convergence
5. Progress plotting of parameters and loglikehood

* *E-step*: Compute the membership weights $w_{ik}$ for all data points, and all mixture components. 
  * This is proportionate to the likelihood of the data point under a particular cluster assignment, multiplied by the weight of that cluster.
$$ p_k(x_i|\theta_k).\alpha_k $$
  * P is a distribution with parameters \$ \theta \$. If Gaussian, then \$ p=\mathcal{N} \$ and \$ \theta_k \$: mean \$ \mu_k \$ and covariance \$ \Sigma_k\$.
  * We divide by a normalizing constant to ensure all weights sum to 1:

$$ w_{ik} = \frac{p_k(x_i|\theta_k).\alpha_k}{\sum_{m=1}^{K}p_m(x_i|\theta_m).\alpha_m}$$ 

{% highlight python %}
from scipy.stats import multivariate_normal as mvn

def compute_membership_weights(data, cluster_weights, means, covariances):
  ndata = len(data)
  nclusters = len(means)
  member_weights = np.zeros((ndata, nclusters))

  for i in range(ndata):
    for k in range(nclusters):
      member_weights[i, k] = cluster_weights[k]*mvn.pdf(data[i], mean=means[k], cov=covariances[k])
    
  row_sums = member_weights.sum(axis=1).reshape(-1, 1) #transpose
  member_weights = member_weights/row_sums

  return member_weights
{% endhighlight %}

* *M-step*: Use the membership weights and the data to calculate new parameter values. 
1. Calculating cluster weights
$$ \alpha_k^{new} = \frac{N_k}{N}, 1 \leq k \leq K $$

{% highlight python %}
def get_cluster_weights(member_weights):
  return np.sum(member_weights, axis=0)/len(member_weights)

{% endhighlight %}

2. Calculate mean $\mu_k$, and variance $\Sigma_k$ of the mixture component

$$ \mu_k^{new} = (\frac{1}{N_k})\sum_{i=1}^{N}w_{ik}.x_i, 1 \leq k \leq K $$

$$ \Sigma_k^{new} = (\frac{1}{N_k})\sum_{i=1}^{N}w_{ik}.(x_i - \mu_k^{new})(x_i - \mu_k^{new})^t$$

After computing the parameters, we can recompute the membership weights with the E-step, and you get the idea. Repeat until convergence.
 
### Convergence of the EM Algorithm
Convergence is when there is no significant change (or below threshold) for the log-likelihood after each iteration. Log likelihood is given by:

$$ \sum_{i=1}^{N}logp(x_i|\theta) = \sum_{i=1}^{N}(log\sum_{k=1}^{K}\alpha_{k}p_k(x_i|z_k, \theta_k)) $$



